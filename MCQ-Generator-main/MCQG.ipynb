{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1- Import the text file/article that has to be used for MCQ generation\n",
    "\n",
    "file=open(\"Articles/ahistoryofdepression.txt\", encoding='utf-8') #\"r\" deontes read version open\n",
    "text=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('popular')\n",
    "#Importing the needed files and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2- Extract the important words(keywords) from the text article that can be used to create MCQ using PKE (Python Keyword Extraction)\n",
    "\n",
    "import pke\n",
    "from nltk.corpus import stopwords #Stopwords are the words that we need to avoid while considering keyword extraction\n",
    "import string\n",
    "def getImportantWords(art): \n",
    "    extractor=pke.unsupervised.MultipartiteRank() #Using the Multipartite Unsupervised Extractor as our extractor\n",
    "    extractor.load_document(input=art,language='en')\n",
    "    pos={'PROPN'} #We are only considering proper nouns as valid candidates for our keywords\n",
    "    stops=list(string.punctuation) #Stoplist contains the words to be avoided\n",
    "    stops+=['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-'] #These stand for the brackets as in lrb=left round bracket=\"(\" and so on\n",
    "    stops+=stopwords.words('english')\n",
    "    extractor.candidate_selection(pos=pos) #Sets the candidate selection criteria, as in, which should be considered and which should be avoided\n",
    "    extractor.candidate_weighting() #Sets the preference criteria for the candidates\n",
    "    result=[] \n",
    "    ex=extractor.get_n_best(n=25) #Gets the 25 best candidates according to the criteria set\n",
    "    for each in ex:\n",
    "        result.append(each[0]) \n",
    "    return result\n",
    "impWords=getImportantWords(text) #Get the important words (keywords) from the text article using the above function\n",
    "#print(impWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3- Split the whole text article into an array/list of individual sentences. This will help us fetch the sentences related to the keywords easily\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "def splitTextToSents(art):\n",
    "    s=[sent_tokenize(art)]\n",
    "    s=[y for x in s for y in x]\n",
    "    s=[sent.strip() for sent in s if len(sent)>15] #Removes all the sentences that have length less than 15 so that we can ensure that our questions have enough length for context\n",
    "    return s\n",
    "sents=splitTextToSents(text) #Achieve a well splitted set of sentences from the text article\n",
    "#print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4- Map the sentences which contain the keywords to the related keywords so that we can easily lookup the sentences related to the keywords\n",
    "\n",
    "from flashtext import KeywordProcessor\n",
    "def mapSents(impWords,sents):\n",
    "    processor=KeywordProcessor() #Using keyword processor as our processor for this task\n",
    "    keySents={}\n",
    "    for word in impWords:\n",
    "        keySents[word]=[]\n",
    "        processor.add_keyword(word) #Adds key word to the processor\n",
    "    for sent in sents:\n",
    "        found=processor.extract_keywords(sent) #Extract the keywords in the sentence\n",
    "        for each in found:\n",
    "            keySents[each].append(sent) #For each keyword found, map the sentence to the keyword\n",
    "    for key in keySents.keys():\n",
    "        temp=keySents[key]\n",
    "        temp=sorted(temp,key=len,reverse=True) #Sort the sentences according to their decreasing length in order to ensure the quality of question for the MCQ \n",
    "        keySents[key]=temp\n",
    "    return keySents\n",
    "mappedSents=mapSents(impWords,sents) #Achieve the sentences that contain the keywords and map those sentences to the keywords using this function\n",
    "#print(mappedSents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 35.859684228897095 secs.\n"
     ]
    }
   ],
   "source": [
    "#Step 5- Get the sense of the word. In order to attain a quality set of distractors we need to get the right sense of the keyword. This is explained in detail in the seperate alogrithm documentation\n",
    "\n",
    "from pywsd.similarity import max_similarity\n",
    "from pywsd.lesk import adapted_lesk\n",
    "from pywsd.lesk import simple_lesk\n",
    "from pywsd.lesk import cosine_lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "def getWordSense(sent,word):\n",
    "    word=word.lower() \n",
    "    if len(word.split())>0: #Splits the word with underscores(_) instead of spaces if there are multiple words\n",
    "        word=word.replace(\" \",\"_\")\n",
    "    synsets=wn.synsets(word,'n') #Sysnets from Google are invoked\n",
    "    if synsets:\n",
    "        wup=max_similarity(sent,word,'wup',pos='n')\n",
    "        adapted_lesk_output = adapted_lesk(sent, word, pos='n')\n",
    "        lowest_index=min(synsets.index(wup),synsets.index(adapted_lesk_output))\n",
    "        return synsets[lowest_index]\n",
    "    else:\n",
    "        return None\n",
    "#print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6- Get distractor from WordNet. These distractors work on the basis of hypernym and hyponym explained in detail in the documentation.\n",
    "\n",
    "def getDistractors(syn,word):\n",
    "    dists=[]\n",
    "    word=word.lower()\n",
    "    actword=word\n",
    "    if len(word.split())>0: #Splits the word with underscores(_) instead of spaces if there are multiple words\n",
    "        word.replace(\" \",\"_\")\n",
    "    hypernym = syn.hypernyms() #Gets the hypernyms of the word\n",
    "    if len(hypernym)==0: #If there are no hypernyms for the current word, we simple return the empty list of distractors\n",
    "        return dists\n",
    "    for each in hypernym[0].hyponyms(): #Other wise we find the relevant hyponyms for the hypernyms\n",
    "        name=each.lemmas()[0].name()\n",
    "        if(name==actword):\n",
    "            continue\n",
    "        name=name.replace(\"_\",\" \")\n",
    "        name=\" \".join(w.capitalize() for w in name.split())\n",
    "        if name is not None and name not in dists: #If the word is not already present in the list and is different from he actial word\n",
    "            dists.append(name)\n",
    "    return dists\n",
    "#print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7- The primary goal of this step is to take our MCQ quality one step further. The WordNet might some times fail to produce a hypernym for some words.\n",
    "#In that case the ConcepNet comes into play as they help achieve our distractors when there are no hypernyms present for it in the WordNet. More about this is discussed\n",
    "#in the algorithm documentation.\n",
    "\n",
    "import requests\n",
    "import json\n",
    "def getDistractors2(word):\n",
    "    word=word.lower()\n",
    "    actword=word\n",
    "    if len(word.split())>0: #Splits the word with underscores(_) instead of spaces if there are multiple words\n",
    "        word=word.replace(\" \",\"_\")\n",
    "    dists=[]\n",
    "    url= \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word) #To get ditractors from ConceptNet's API\n",
    "    obj=requests.get(url).json()\n",
    "    for edge in obj['edges']:\n",
    "        link=edge['end']['term']\n",
    "        url2=\"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
    "        obj2=requests.get(url2).json()\n",
    "        for edge in obj2['edges']:\n",
    "            word2=edge['start']['label']\n",
    "            if word2 not in dists and actword.lower() not in word2.lower(): #If the word is not already present in the list and is different from he actial word\n",
    "                dists.append(word2)\n",
    "    return dists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 8- Find and map the distractors to the keywords\n",
    "\n",
    "mappedDists={}\n",
    "for each in mappedSents:\n",
    "    wordsense=getWordSense(mappedSents[each][0],each) #gets the sense of the word\n",
    "    if wordsense: #if the wordsense is not null/none\n",
    "        dists=getDistractors(wordsense,each) #Gets the WordNet distractors\n",
    "        if len(dists)==0: #If there are no WordNet distractors available for the current word\n",
    "            dists=getDistractors2(each) #The gets the distractors from the ConceptNet API\n",
    "        if len(dists)!=0: #If there are indeed distractors from WordNet available, then maps them\n",
    "            mappedDists[each]=dists\n",
    "    else: #If there is no wordsense, the directly searches/uses the ConceptNet\n",
    "        dists=getDistractors2(each)\n",
    "        if len(dists)>0: #If it gets the Distractors then maps them\n",
    "            mappedDists[each]=dists\n",
    "#print(mappedDists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 9- The final step is to present our MCQ in a nice and readable manner.\n",
    "\n",
    "print(\"**************************************        Multiple Choice Questions        *******************************\")\n",
    "print()\n",
    "import re\n",
    "import random\n",
    "iterator = 1 #To keep the count of the questions\n",
    "for each in mappedDists:\n",
    "    sent=mappedSents[each][0]\n",
    "    p=re.compile(each,re.IGNORECASE) #Converts into regular expression for pattern matching\n",
    "    op=p.sub(\"________\",sent) #Replaces the keyword with underscores(blanks)\n",
    "    print(\"Question %s-> \"%(iterator),op) #Prints the question along with a question number\n",
    "    options=[each.capitalize()]+mappedDists[each] #Capitalizes the options\n",
    "    options=options[:4] #Selects only 4 options\n",
    "    opts=['a','b','c','d']\n",
    "    random.shuffle(options) #Shuffle the options so that order is not always same\n",
    "    for i,ch in enumerate(options):\n",
    "        print(\"\\t\",opts[i],\") \", ch) #Print the options\n",
    "    print()\n",
    "    iterator+=1 #Increase the counter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
